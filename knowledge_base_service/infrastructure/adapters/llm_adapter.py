"""
LLM adapter for generating responses using OpenRouter/Gemini
"""
import logging
from typing import List, Optional

try:
    from openai import AsyncOpenAI
except ImportError:
    AsyncOpenAI = None

from config import get_settings

logger = logging.getLogger(__name__)


class LLMAdapter:
    """Adapter for LLM communication via OpenRouter"""
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: Optional[str] = None
    ):
        """
        Initialize LLM adapter
        
        Args:
            api_key: OpenRouter API key (if None, uses settings)
            base_url: OpenRouter base URL (if None, uses settings)
            model: LLM model to use (if None, uses settings)
        """
        if AsyncOpenAI is None:
            raise ImportError("openai package is required. Install with: pip install openai")
        
        # Load settings dynamically to get latest values from .env
        settings = get_settings()
        
        # Get the .env file path from config
        import config
        env_path = config.ENV_FILE_PATH if hasattr(config, 'ENV_FILE_PATH') and config.ENV_FILE_PATH.exists() else None
        
        self.api_key = api_key or settings.openrouter_api_key
        self.base_url = base_url or settings.openrouter_base_url
        self.model = model or getattr(settings, 'llm_model', 'google/gemini-2.5-flash-lite-preview-09-2025')
        
        if not self.api_key:
            logger.warning("No OpenRouter API key provided. LLM responses will be disabled.")
            if env_path:
                logger.warning(f"Expected .env file location: {env_path}")
            logger.warning("Make sure OPENROUTER_API_KEY is set in the .env file in the PROJECT ROOT directory")
            logger.warning("(Not in knowledge_base_service folder, but in the main TravelAssistant folder)")
            logger.warning("Format: OPENROUTER_API_KEY=sk-or-v1-... (no spaces around =)")
            self.client = None
        else:
            # Mask API key in logs (show only first 8 chars)
            masked_key = self.api_key[:8] + "..." if len(self.api_key) > 8 else "***"
            logger.info(f"OpenRouter API key loaded (key: {masked_key})")
            self.client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
    
    async def generate_response(
        self,
        user_query: str,
        context_documents: List[dict],
        conversation_history: Optional[List[dict]] = None
    ) -> str:
        """
        Generate response to user query based on context documents
        
        Args:
            user_query: User's question
            context_documents: List of documents from search results with 'content' and optional 'metadata'
            conversation_history: Previous conversation messages (optional)
            
        Returns:
            Generated response text
        """
        if not self.client:
            return "Przepraszam, ale funkcja generowania odpowiedzi przez LLM nie jest dostÄ™pna. Brak klucza API OpenRouter."
        
        # Build context from documents
        context_text = self._build_context(context_documents)
        
        # Log context for debugging
        logger.info(f"ğŸ“ Generowanie odpowiedzi LLM")
        logger.info(f"   Query: {user_query[:100]}..." if len(user_query) > 100 else f"   Query: {user_query}")
        logger.info(f"   Liczba dokumentÃ³w kontekstowych: {len(context_documents)}")
        if context_documents:
            logger.debug(f"   Pierwszy dokument (pierwsze 200 znakÃ³w): {context_documents[0].get('content', '')[:200]}...")
        if conversation_history:
            logger.info(f"   Historia konwersacji: {len(conversation_history)} wiadomoÅ›ci")
            for idx, msg in enumerate(conversation_history[-3:], 1):  # Log last 3 messages
                role = msg.get("role", "unknown")
                content_preview = msg.get("content", "")[:50] + "..." if len(msg.get("content", "")) > 50 else msg.get("content", "")
                logger.debug(f"      {idx}. {role}: {content_preview}")
        
        # Build prompt
        system_prompt = self._build_system_prompt()
        user_prompt = self._build_user_prompt(user_query, context_text)
        
        try:
            # Build messages list in correct order:
            # 1. System prompt
            # 2. Conversation history (if any)
            # 3. Current user query with context
            messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            # Add conversation history BEFORE current query (if provided)
            if conversation_history:
                # Add all history messages (they should already exclude current query)
                for msg in conversation_history:
                    if msg.get("role") in ["user", "assistant"]:
                        content = msg.get("content", "")
                        if content:  # Only add non-empty messages
                            messages.append({
                                "role": msg["role"],
                                "content": content
                            })
            
            # Add current user query with context (always last)
            messages.append({"role": "user", "content": user_prompt})
            
            logger.debug(f"   WysyÅ‚am {len(messages)} wiadomoÅ›ci do LLM (system + {len(conversation_history) if conversation_history else 0} historia + 1 aktualne pytanie)")
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,  # type: ignore
                temperature=0.7,
                max_tokens=2000  # ZwiÄ™kszony limit dla dÅ‚uÅ¼szych odpowiedzi z kontekstem
            )
            
            content = response.choices[0].message.content
            response_text = content.strip() if content else "Przepraszam, nie udaÅ‚o siÄ™ wygenerowaÄ‡ odpowiedzi."
            
            logger.info(f"âœ… OdpowiedÅº LLM wygenerowana (dÅ‚ugoÅ›Ä‡: {len(response_text)} znakÃ³w)")
            logger.debug(f"   OdpowiedÅº (pierwsze 200 znakÃ³w): {response_text[:200]}...")
            
            return response_text
            
        except Exception as e:
            logger.error(f"Error generating LLM response: {e}", exc_info=True)
            return f"Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d podczas generowania odpowiedzi: {str(e)}"
    
    def _build_context(self, documents: List[dict]) -> str:
        """Build context string from search results"""
        if not documents:
            return "Brak dostÄ™pnych dokumentÃ³w w bazie wiedzy."
        
        context_parts = []
        for idx, doc in enumerate(documents, 1):
            content = doc.get("content", "")
            metadata = doc.get("metadata", {})
            
            # Build document entry
            doc_entry = f"[Dokument {idx}]"
            if metadata.get("filename"):
                doc_entry += f" Å¹rÃ³dÅ‚o: {metadata.get('filename')}"
            if metadata.get("document_id"):
                doc_entry += f" (ID: {metadata.get('document_id')})"
            
            # Increase content length limit to 2000 chars for better context
            doc_entry += f"\n{content[:2000]}"
            if len(content) > 2000:
                doc_entry += "..."
            
            context_parts.append(doc_entry)
        
        return "\n\n".join(context_parts)
    
    def _build_system_prompt(self) -> str:
        """Build system prompt for LLM"""
        return """JesteÅ› pomocnym asystentem podrÃ³Å¼y. Twoim zadaniem jest odpowiadaÄ‡ na pytania uÅ¼ytkownikÃ³w na podstawie dostÄ™pnych dokumentÃ³w z bazy wiedzy oraz historii konwersacji.

Zasady:
1. Odpowiadaj na podstawie informacji zawartych w dostarczonych dokumentach oraz historii konwersacji
2. JeÅ›li informacja nie jest dostÄ™pna w dokumentach ani w historii, powiedz to jasno
3. Odpowiadaj w jÄ™zyku polskim, naturalnie i przyjaÅºnie
4. Strukturyzuj odpowiedzi, uÅ¼ywajÄ…c punktÃ³w lub krÃ³tkich akapitÃ³w
5. JeÅ›li dokumenty zawierajÄ… konkretne liczby, daty lub fakty, uÅ¼yj ich dokÅ‚adnie
6. Nie wymyÅ›laj informacji, ktÃ³rych nie ma w dokumentach ani w historii konwersacji
7. JeÅ›li pytanie dotyczy konkretnego miejsca, skup siÄ™ na informacjach z dokumentÃ³w dotyczÄ…cych tego miejsca
8. Wykorzystuj historiÄ™ konwersacji jako kontekst - moÅ¼esz odwoÅ‚ywaÄ‡ siÄ™ do wczeÅ›niejszych pytaÅ„ i odpowiedzi
9. JeÅ›li uÅ¼ytkownik zadaje pytanie powiÄ…zane z poprzednimi, uÅ¼yj zarÃ³wno dokumentÃ³w jak i historii konwersacji do udzielenia peÅ‚nej odpowiedzi"""
    
    def _build_user_prompt(
        self,
        user_query: str,
        context_text: str
    ) -> str:
        """Build user prompt with context"""
        prompt = f"""Na podstawie poniÅ¼szych dokumentÃ³w z bazy wiedzy oraz historii konwersacji (jeÅ›li dostÄ™pna), odpowiedz na pytanie uÅ¼ytkownika.

AKTUALNE PYTANIE UÅ»YTKOWNIKA:
{user_query}

DOSTÄ˜PNE DOKUMENTY Z BAZY WIEDZY:
{context_text}

INSTRUKCJE:
- Odpowiedz na pytanie: "{user_query}"
- UÅ¼yj informacji z powyÅ¼szych dokumentÃ³w oraz z historii konwersacji (jeÅ›li jest dostÄ™pna)
- JeÅ›li pytanie jest powiÄ…zane z wczeÅ›niejszymi pytaniami z historii, moÅ¼esz odwoÅ‚aÄ‡ siÄ™ do nich
- JeÅ›li dokumenty nie zawierajÄ… odpowiedzi, sprawdÅº czy informacja nie byÅ‚a juÅ¼ omÃ³wiona w historii konwersacji
- JeÅ›li informacja nie jest dostÄ™pna ani w dokumentach ani w historii, powiedz to jasno
- Odpowiadaj w sposÃ³b naturalny i pomocny, wykorzystujÄ…c caÅ‚y dostÄ™pny kontekst

Odpowiedz na pytanie uÅ¼ytkownika w sposÃ³b naturalny i pomocny, wykorzystujÄ…c zarÃ³wno dokumenty jak i historiÄ™ konwersacji."""
        
        return prompt

